references.bib

@INPROCEEDINGS{ptolemy,
  author={Gan, Yiming and Qiu, Yuxian and Leng, Jingwen and Guo, Minyi and Zhu, Yuhao},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Ptolemy: Architecture Support for Robust Deep Learning}, 
  year={2020},
  volume={},
  number={},
  pages={241-255},
  doi={10.1109/MICRO50266.2020.00031}}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@inproceedings{ensembles, author = {He, Warren and Wei, James and Chen, Xinyun and Carlini, Nicholas and Song, Dawn}, title = {Adversarial Example Defenses: Ensembles of Weak Defenses Are Not Strong}, year = {2017}, publisher = {USENIX Association}, address = {USA}, abstract = {Ongoing research has proposed several methods to defend neural networks against adversarial examples, many of which researchers have shown to be ineffective. We ask whether a strong defense can be created by combining multiple (possibly weak) defenses. To answer this question, we study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion. Thus, our work implies that ensemble of weak defenses is not sufficient to provide strong defense against adversarial examples.}, booktitle = {Proceedings of the 11th USENIX Conference on Offensive Technologies}, pages = {15}, numpages = {1}, location = {Vancouver, BC, Canada}, series = {WOOT'17} }
@inbook{detected, author = {Carlini, Nicholas and Wagner, David}, title = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods}, year = {2017}, isbn = {9781450352024}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3128572.3140444}, abstract = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.}, booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security}, pages = {3â€“14}, numpages = {12} }