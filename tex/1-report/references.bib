references.bib

@INPROCEEDINGS{ptolemy,
  author={Gan, Yiming and Qiu, Yuxian and Leng, Jingwen and Guo, Minyi and Zhu, Yuhao},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Ptolemy: Architecture Support for Robust Deep Learning}, 
  year={2020},
  volume={},
  number={},
  pages={241-255},
  doi={10.1109/MICRO50266.2020.00031}}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@inproceedings{ensembles, author = {He, Warren and Wei, James and Chen, Xinyun and Carlini, Nicholas and Song, Dawn}, title = {Adversarial Example Defenses: Ensembles of Weak Defenses Are Not Strong}, year = {2017}, publisher = {USENIX Association}, address = {USA}, abstract = {Ongoing research has proposed several methods to defend neural networks against adversarial examples, many of which researchers have shown to be ineffective. We ask whether a strong defense can be created by combining multiple (possibly weak) defenses. To answer this question, we study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion. Thus, our work implies that ensemble of weak defenses is not sufficient to provide strong defense against adversarial examples.}, booktitle = {Proceedings of the 11th USENIX Conference on Offensive Technologies}, pages = {15}, numpages = {1}, location = {Vancouver, BC, Canada}, series = {WOOT'17} }
@inbook{detected, author = {Carlini, Nicholas and Wagner, David}, title = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods}, year = {2017}, isbn = {9781450352024}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3128572.3140444}, abstract = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.}, booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security}, pages = {3–14}, numpages = {12} }


@article{Goodfellow2015adversarial,
  title={Explaining and Harnessing Adversarial Examples},
  author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6572}
}

@article{2019glasses,
   title={A General Framework for Adversarial Examples with Objectives},
   volume={22},
   ISSN={2471-2574},
   url={http://dx.doi.org/10.1145/3317611},
   DOI={10.1145/3317611},
   number={3},
   journal={ACM Transactions on Privacy and Security},
   publisher={Association for Computing Machinery (ACM)},
   author={Sharif, Mahmood and Bhagavatula, Sruti and Bauer, Lujo and Reiter, Michael K.},
   year={2019},
   month={Jul},
   pages={1–30}
}

@misc{brown2018adversarial,
      title={Adversarial Patch}, 
      author={Tom B. Brown and Dandelion Mané and Aurko Roy and Martín Abadi and Justin Gilmer},
      year={2018},
      eprint={1712.09665},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{stopsign,
author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
year = {2018},
month = {06},
pages = {1625-1634},
title = {Robust Physical-World Attacks on Deep Learning Visual Classification},
doi = {10.1109/CVPR.2018.00175}
}

@inproceedings{
buckman2018thermometer,
title={Thermometer Encoding: One Hot Way To Resist Adversarial Examples},
author={Jacob Buckman and Aurko Roy and Colin Raffel and Ian Goodfellow},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S18Su--CW},
}

@article{onrecent,
  author    = {Tao Bai and
               Jinqi Luo and
               Jun Zhao and
               Bihan Wen and
               Qian Wang},
  title     = {Recent Advances in Adversarial Training for Adversarial Robustness},
  journal   = {CoRR},
  volume    = {abs/2102.01356},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.01356},
  eprinttype = {arXiv},
  eprint    = {2102.01356},
  timestamp = {Mon, 07 Jun 2021 11:11:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-01356.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{whitebox,
  author    = {Anish Athalye and
               Nicholas Carlini},
  title     = {On the Robustness of the {CVPR} 2018 White-Box Adversarial Example
               Defenses},
  journal   = {CoRR},
  volume    = {abs/1804.03286},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.03286},
  eprinttype = {arXiv},
  eprint    = {1804.03286},
  timestamp = {Mon, 13 Aug 2018 16:47:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-03286.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{deepfense,
author = {Rouhani, Bita Darvish and Samragh, Mohammad and Javaheripi, Mojan and Javidi, Tara and Koushanfar, Farinaz},
title = {DeepFense: Online Accelerated Defense against Adversarial Deep Learning},
year = {2018},
isbn = {9781450359504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240765.3240791},
doi = {10.1145/3240765.3240791},
abstract = {Recent advances in adversarial Deep Learning (DL) have opened up a largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. With the wide-spread usage of DL in critical and time-sensitive applications, including unmanned vehicles, drones, and video surveillance systems, online detection of malicious inputs is of utmost importance. We propose DeepFense, the first end-to-end automated framework that simultaneously enables efficient and safe execution of DL models. DeepFense formalizes the goal of thwarting adversarial attacks as an optimization problem that minimizes the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples in parallel with the victim DL model. DeepFense leverages hardware/software/algorithm co-design and customized acceleration to achieve just-in-time performance in resource-constrained settings. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. We further provide an accompanying API to reduce the non-recurring engineering cost and ensure automated adaptation to various platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders of magnitude performance improvement while enabling online adversarial sample detection.},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
articleno = {134},
numpages = {8},
keywords = {adversarial attacks, model reliability, deep learning, real-time computing, FPGA acceleration},
location = {San Diego, California},
series = {ICCAD '18}
}

@article{imagetransformation,
  title={Image Transformation can make Neural Networks more robust against Adversarial Examples},
  author={Dang Duy Thang and Toshihiro Matsui},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.03037}
}

@INPROCEEDINGS{identify,  author={Wang, Yulong and Su, Hang and Zhang, Bo and Hu, Xiaolin},  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},   title={Interpret Neural Networks by Identifying Critical Data Routing Paths},   year={2018},  volume={},  number={},  pages={8906-8914},  doi={10.1109/CVPR.2018.00928}}
@INPROCEEDINGS{imagenet,  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},   title={ImageNet: A large-scale hierarchical image database},   year={2009},  volume={},  number={},  pages={248-255},  doi={10.1109/CVPR.2009.5206848}}
@inproceedings{alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1097–1105},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@misc{resnet18,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{10.1145/3079856.3080246,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080246},
doi = {10.1145/3079856.3080246},
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {1–12},
numpages = {12},
keywords = {domain-specific architecture, accelerator, GPU, MLP, TPU, LSTM, deep learning, CNN, TensorFlow, DNN, RNN, neural network},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{tpu,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080246},
doi = {10.1145/3140659.3080246},
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {1–12},
numpages = {12},
keywords = {MLP, LSTM, accelerator, neural network, TPU, RNN, deep learning, domain-specific architecture, DNN, TensorFlow, CNN, GPU}
}




@misc{icarus,
  title = {Icarus Verilog},
  author = {Steve Williams},
  howpublished = {\url{http://iverilog.icarus.com}},
  note = {Accessed: 2021-12-07}
}
@inproceedings{yosys,
  title={Yosys-A Free Verilog Synthesis Suite},
  author={Clifford Wolf and Johann Glaser and Johannes Kepler},
  year={2013}
}
@article{hp-cacti, author = {Balasubramonian, Rajeev and Kahng, Andrew B. and Muralimanohar, Naveen and Shafiee, Ali and Srinivas, Vaishnav}, title = {CACTI 7: New Tools for Interconnect Exploration in Innovative Off-Chip Memories}, year = {2017}, issue_date = {July 2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {14}, number = {2}, issn = {1544-3566}, url = {https://doi.org/10.1145/3085572}, doi = {10.1145/3085572}, abstract = {Historically, server designers have opted for simple memory systems by picking one of a few commoditized DDR memory products. We are already witnessing a major upheaval in the off-chip memory hierarchy, with the introduction of many new memory products—buffer-on-board, LRDIMM, HMC, HBM, and NVMs, to name a few. Given the plethora of choices, it is expected that different vendors will adopt different strategies for their high-capacity memory systems, often deviating from DDR standards and/or integrating new functionality within memory systems. These strategies will likely differ in their choice of interconnect and topology, with a significant fraction of memory energy being dissipated in I/O and data movement. To make the case for memory interconnect specialization, this paper makes three contributions.First, we design a tool that carefully models I/O power in the memory system, explores the design space, and gives the user the ability to define new types of memory interconnects/topologies. The tool is validated against SPICE models, and is integrated into version 7 of the popular CACTI package. Our analysis with the tool shows that several design parameters have a significant impact on I/O power.We then use the tool to help craft novel specialized memory system channels. We introduce a new relay-on-board chip that partitions a DDR channel into multiple cascaded channels. We show that this simple change to the channel topology can improve performance by 22% for DDR DRAM and lower cost by up to 65% for DDR DRAM. This new architecture does not require any changes to DIMMs, and it efficiently supports hybrid DRAM/NVM systems.Finally, as an example of a more disruptive architecture, we design a custom DIMM and parallel bus that moves away from the DDR3/DDR4 standards. To reduce energy and improve performance, the baseline data channel is split into three narrow parallel channels and the on-DIMM interconnects are operated at a lower frequency. In addition, this allows us to design a two-tier error protection strategy that reduces data transfers on the interconnect. This architecture yields a performance improvement of 18% and a memory power reduction of 23%.The cascaded channel and narrow channel architectures serve as case studies for the new tool and show the potential for benefit from re-organizing basic memory interconnects.}, journal = {ACM Trans. Archit. Code Optim.}, month = {jun}, articleno = {14}, numpages = {25}, keywords = {tools, interconnects, NVM, DRAM, Memory} }